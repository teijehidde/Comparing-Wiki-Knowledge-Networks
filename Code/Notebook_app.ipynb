{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# Setup packages\n",
    "import os\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import collections \n",
    "import networkx as nx\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# all these network algorithms are currently being tried out. \n",
    "from networkx.algorithms import approximation\n",
    "from networkx.algorithms import community\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "# For network visualization: \n",
    "from pyvis.network import Network\n",
    "# use dynetx for dynamic network visualization? -- when I can download and incoporate revision history? \n",
    "\n",
    "PATH = \"/home/teijehidde/Documents/Git Blog and Coding/data_dump/\"\n",
    "DATA_FILE = \"DATA.json\" \n",
    "\n",
    "# Loading JSON file: \n",
    "with open(PATH + DATA_FILE) as json_file:\n",
    "    network_data = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "def SaveData(wiki_data, node_title, lang):\n",
    "    # step 1: transforming data from API into unified dictionary. \n",
    "    # 1a: creating list of available nodes. \n",
    "    all_nodes = []\n",
    "\n",
    "    for item in wiki_data:\n",
    "        all_nodes = all_nodes + list(item['query']['pages'].keys())\n",
    "    all_nodes = list(set(all_nodes))\n",
    "\n",
    "    # 1b: Using all_nodes to go through raw data from API -- in this case this should just by 1 node. \n",
    "    for node in all_nodes:\n",
    "        node_data = {'node_ID': node, 'title': '', 'links': [], 'ego': [], 'language': lang, 'AvailableLanguages': []}\n",
    "        \n",
    "        item_name = lang + node\n",
    "        if item_name in network_data.keys():\n",
    "            node_data = network_data[item_name]\n",
    "        \n",
    "        for item in wiki_data:\n",
    "            if node in item['query']['pages'].keys(): \n",
    "                node_data['title'] = item['query']['pages'][node]['title']\n",
    "\n",
    "                if 'links' in item['query']['pages'][node].keys():\n",
    "                    for link in item['query']['pages'][node]['links']: \n",
    "                        node_data['links'].append(link['title'])\n",
    "\n",
    "                if 'langlinks' in item['query']['pages'][node].keys():\n",
    "                    node_data['AvailableLanguages'] = item['query']['pages'][node]['langlinks']\n",
    "\n",
    "                node_data['ego'].append(node_title)\n",
    "        \n",
    "        node_data['ego'] = list(set(node_data['ego']))\n",
    "        \n",
    "        network_data[lang + node] = node_data\n",
    "\n",
    "    # Step 2: Saving data to json file. \n",
    "    try: \n",
    "        with open(PATH + DATA_FILE, 'w') as outfile:\n",
    "            json.dump(network_data, outfile)\n",
    "            print(\"Data succesfully saved. Wiki node name: \" + node_title + \"; downloaded in language: \" + lang + \".\")\n",
    "\n",
    "    except: \n",
    "        print(\"Something went wrong. Check code.\")\n",
    "\n",
    "# optional, for debugging: \n",
    "#   finally:\n",
    "#       return wiki_data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# Function: download additional languages of ego network.\n",
    "def downloadAdditionalLanguage(node_title, original_lang, additional_langs = []): # \"de\", \"fr\", \"nl\"\n",
    "\n",
    "    # download data fro JSON file. \n",
    "    with open(PATH + DATA_FILE) as json_file:\n",
    "        network_data = json.load(json_file)\n",
    "    \n",
    "    # make a list of the language that are available for requested page. \n",
    "    available_languages = [v['AvailableLanguages'] for (k,v) in network_data.items() if v['title'] == node_title if v['language'] == original_lang][0]\n",
    "    list_available_languages = []\n",
    "    for item in available_languages: \n",
    "         list_available_languages.append(item['lang'])\n",
    "\n",
    "    # If no languages are requested, the function shows available languages. \n",
    "    if additional_langs == []:\n",
    "        pass\n",
    "\n",
    "    if additional_langs == [\"available_langs\"]:\n",
    "        print('The wikipedia page is available in the following languages:')         \n",
    "        print(list_available_languages)\n",
    "    \n",
    "    # Goes through avialble languages of a wikipedia page, and downloads those that were requested (using the downloadNetwork() function). \n",
    "    else:\n",
    "        for item in available_languages: \n",
    "            if item['lang'] in additional_langs:\n",
    "                downloadNetworks(node_title = item['*'], original_lang = item['lang'], additional_langs = [])\n",
    "    \n",
    "        print(\"Download of additional languages finished.\") \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Function: download data on links from ALL PAGES linked to 'node_title' (but excluding node_title itself!) from Wikimedia API and save to json file.  \n",
    "def downloadNetworks(node_title, original_lang = \"en\", additional_langs = [\"ar\" \"de\", \"fr\", \"nl\"]): \n",
    "\n",
    "    # setup and load existing data node.\n",
    "    API_ENDPOINT = \"https://\" + original_lang + \".wikipedia.org/w/api.php\" # fr.wikipedia.org; https://en.wikipedia.org\n",
    "    wiki_data = []\n",
    "\n",
    "    # step 1: download data on the central node of the network (incl. available languages). \n",
    "    # setup API query and initial API call \n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": node_title,\n",
    "        \"prop\": \"links|info|langlinks\",\n",
    "        \"plnamespace\": 0, \n",
    "        \"pllimit\": 500,\n",
    "        \"lllimit\": 500, \n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "    wiki_data.append(response.json())\n",
    "    \n",
    "    # Continue API call until all data on ego node has been downloaded. \n",
    "    while 'continue' in wiki_data[-1].keys():\n",
    "        \n",
    "        PARAMS_CONT = PARAMS\n",
    "        PARAMS_CONT[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue']\n",
    "\n",
    "        response = S.get(url=API_ENDPOINT, params=PARAMS_CONT)\n",
    "        wiki_data.append(response.json())\n",
    "\n",
    "    # step 2: use generator to download data on all additional nodes. \n",
    "    # setup API query for first generator API call (used to download data on all pages that are linked to node_title) \n",
    "    print(\"Downloading Wiki network name: \" + node_title + \" in language: \" + original_lang + \". Please note that this can take a while.\")\n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"generator\": \"links\",\n",
    "        \"titles\": node_title,\n",
    "        \"gplnamespace\": 0, \n",
    "        \"gpllimit\": 500, \n",
    "        \"plnamespace\": 0,\n",
    "        \"pllimit\": 500, \n",
    "        \"prop\": \"links\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "    wiki_data.append(response.json())\n",
    "\n",
    "    # Continue API call until all data on network is downloaded. \n",
    "    while 'continue' in wiki_data[-1].keys():\n",
    "\n",
    "        PARAMS_CONT = PARAMS\n",
    "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
    "            PARAMS_CONT[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
    "\n",
    "        if 'gplcontinue' in wiki_data[-1]['continue']: \n",
    "            PARAMS_CONT[\"gplcontinue\"] = wiki_data[-1]['continue']['gplcontinue']\n",
    "\n",
    "        response = S.get(url=API_ENDPOINT, params = PARAMS_CONT)\n",
    "        wiki_data.append(response.json())\n",
    "\n",
    "    # step 3: transform and save data:  \n",
    "    SaveData(wiki_data, node_title=node_title, lang=original_lang)\n",
    "\n",
    "    # step 4: download additional languages: \n",
    "    downloadAdditionalLanguage(node_title = node_title, original_lang = original_lang, additional_langs = additional_langs)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# function: provide titles of networks that are saved in the JSON file. Also provides the language they were saved in. \n",
    "def getDownloadedNetworks(): \n",
    "\n",
    "    # download data from JSON file. \n",
    "    with open(PATH + DATA_FILE) as json_file:\n",
    "        network_data = json.load(json_file)\n",
    "    \n",
    "    # create set of ego network names.  \n",
    "    downloaded_networks = [(v['ego']) for (k,v) in network_data.items()]\n",
    "    downloaded_networks = set(list(chain(*downloaded_networks)))\n",
    "    downloaded_networks = [v for (k,v) in network_data.items() if v['title'] in downloaded_networks]\n",
    "\n",
    "    # print names of ego networks and language that they have been downloaded in. \n",
    "    items = {}  \n",
    "    for network in downloaded_networks: \n",
    "        items[network['title'] + ' (' + network['language'] + ')'] = {'lang':  network['language'], '*': network['title']}    \n",
    "    return(items)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Initiate class Node. \n",
    "class WikiNode:\n",
    "    def __init__(self, node_title, lang):\n",
    "        \n",
    "        # Select node in JSON file (by title and language). \n",
    "        node_data = [v for (k,v) in network_data.items() if v['title'] == node_title if v['language'] == lang][0]\n",
    "        \n",
    "        # Extract data and place in instance of Wikinode class. \n",
    "        self.node_title = node_data['title']\n",
    "        self.node_ID = node_data['node_ID']\n",
    "        self.node_links = node_data['links']\n",
    "        self.node_lang = node_data['language']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Initiate class WikiNetwork\n",
    "class WikiNetwork(WikiNode):\n",
    "   \n",
    "    def __init__(self,node_title, lang):\n",
    "        \n",
    "        # initiate the central node of the network as class WikiNode, add additional attributes for class WikiNetwork \n",
    "        WikiNode.__init__(self, node_title, lang)\n",
    "        self.network_nodes = {}\n",
    "        self.network_links = []\n",
    "        self.network_edges = [] \n",
    "        self.network_status = []\n",
    "    \n",
    "        # Go through node_links of the central node (node_title) to build network.\n",
    "        for link in self.node_links:\n",
    "            try:     \n",
    "                Node2 = WikiNode(link, lang)             \n",
    "                purged_links = [x for x in Node2.node_links if x in self.node_links]\n",
    "                purged_edges = []\n",
    "                for purged_link in purged_links:\n",
    "                    purged_edges.append((link,purged_link))  \n",
    "                self.network_nodes[Node2.node_ID] = Node2\n",
    "                self.network_links = self.network_links + purged_links\n",
    "                self.network_edges = self.network_edges + purged_edges\n",
    "            except: \n",
    "                print('Loading of node ' + link + ' failed.')\n",
    "            self.links_count = Counter(self.network_links)\n",
    "\n",
    "    def getNodes(self, type=\"cytoscape\", threshold=0):\n",
    "        selected_nodes = [k for k,v in self.links_count.items() if float(v) >= threshold]\n",
    "        \n",
    "        if type == 'networkx':\n",
    "            return [(i, {\"name\": i}) for i in selected_nodes]\n",
    "\n",
    "        if type == 'cytoscape':\n",
    "            return [{'data': {'id': i, \"label\": i}} for i in selected_nodes]\n",
    "\n",
    "    def getEdges(self,type=\"cytoscape\", threshold=0):  \n",
    "        selected_nodes = [k for k,v in self.links_count.items() if float(v) >= threshold]\n",
    "        edges_network = [(a,b) for a,b in self.network_edges if a in selected_nodes and b in selected_nodes]\n",
    "\n",
    "        if type == 'networkx':\n",
    "            return edges_network\n",
    "\n",
    "        if type == 'cytoscape':\n",
    "            return [{'data': {'source': a, \"target\": b}} for a,b in edges_network]\n",
    "\n",
    "    def getStatsNetwork(self): \n",
    "        # TODO: return a dictionary with stats on network: \n",
    "        # - triangles\n",
    "        # - degree_centrality \n",
    "        # - ... \n",
    "\n",
    "        print('WIP')\n",
    "\n",
    "    def getStatsNodes(self, nodes):\n",
    "        # TODO: return an numpy array with stats per node: \n",
    "        # - triangles\n",
    "        # - degree_centrality \n",
    "        # - ... \n",
    "        # if nodes == None: \n",
    "          #  node = self.node_links\n",
    "\n",
    "        print('WIP')\n",
    "    \n",
    "    def getNetworkCommunities(self,threshold=0):\n",
    "        # TODO: return an numpy array with stats per community. Add in an overall library.  \n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(self.getEdges(threshold))\n",
    "        \n",
    "        return greedy_modularity_communities(G)\n",
    "    \n",
    "    def drawGraph(self,threshold=0,name='no_name'):\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(self.getEdges(threshold))\n",
    "\n",
    "        netdraw = Network('2000px', '2000px')\n",
    "        netdraw.from_nx(G)\n",
    "        netdraw.barnes_hut()\n",
    "\n",
    "        title = name + \".html\"\n",
    "        netdraw.show(title)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Class: a collection of wiki networks of the same topic, in different languages. Automatically takes all languages that have been downloaded before. \n",
    "class WikiNetworkCollection():\n",
    "\n",
    "        def __init__(self,chosen_networks):\n",
    "        \n",
    "                # make list available ego networks\n",
    "                all_downloaded_networks = getDownloadedNetworks()\n",
    "                available_languages = [v['AvailableLanguages'] for (k,v) in network_data.items() if v['title'] == node_title][0]\n",
    "                \n",
    "                topic_networks = []\n",
    "                topic_networks = [{'lang': original_language, '*': node_title}] + [v for v in available_languages if v in all_downloaded_networks]\n",
    "\n",
    "                # initiate tclass WikiNetwork for each available language.  \n",
    "                self.networks = {}\n",
    "                if topic_networks is not []:\n",
    "                        for network in topic_networks:\n",
    "                                self.networks[network['lang'] + '_' + network['*']] = WikiNetwork(node_title = network['*'], language = network['lang'])\n",
    "\n",
    "        def getStatsIsomorphism(self):\n",
    "        # TODO: return an numpy array with stats per network: \n",
    "        # - ... related to similarities / difference of network to other networks in collection. \n",
    "                print('WIP')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "########################################\n",
    "########################################\n",
    "# FROM HERE RUN TIME STARTS # "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "getDownloadedNetworks()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Flask (en)': {'lang': 'en', '*': 'Flask'},\n",
       " 'Kolba (de)': {'lang': 'de', '*': 'Kolba'},\n",
       " 'Royston (en)': {'lang': 'en', '*': 'Royston'},\n",
       " 'Oxford (en)': {'lang': 'en', '*': 'Oxford'},\n",
       " 'Flask (fr)': {'lang': 'fr', '*': 'Flask'},\n",
       " 'Oxford (de)': {'lang': 'de', '*': 'Oxford'},\n",
       " 'Oxford (fr)': {'lang': 'fr', '*': 'Oxford'},\n",
       " 'Vaccine (en)': {'lang': 'en', '*': 'Vaccine'},\n",
       " 'Vaccine (fr)': {'lang': 'fr', '*': 'Vaccine'},\n",
       " 'Vaccine (de)': {'lang': 'de', '*': 'Vaccine'},\n",
       " 'Secularism (en)': {'lang': 'en', '*': 'Secularism'},\n",
       " 'علمانية (ar)': {'lang': 'ar', '*': 'علمانية'},\n",
       " 'Laïcité (fr)': {'lang': 'fr', '*': 'Laïcité'},\n",
       " 'Secularisme (nl)': {'lang': 'nl', '*': 'Secularisme'}}"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "test = WikiNetwork('Oxford', lang = 'en')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "downloadNetworks('Secularism', 'en', ['ar', 'fr', 'nl'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading Wiki network name: Secularism in language: en. Please note that this can take a while.\n",
      "Data succesfully saved. Wiki node name: Secularism; downloaded in language: en.\n",
      "Downloading Wiki network name: علمانية in language: ar. Please note that this can take a while.\n",
      "Data succesfully saved. Wiki node name: علمانية; downloaded in language: ar.\n",
      "The wikipedia page is available in the following languages:\n",
      "['af', 'ary', 'arz', 'ast', 'az', 'be', 'be-x-old', 'bg', 'bh', 'bn', 'br', 'bs', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'diq', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'fr', 'gl', 'he', 'hi', 'hr', 'hy', 'id', 'is', 'it', 'ja', 'ka', 'kn', 'ko', 'la', 'lad', 'lfn', 'li', 'lo', 'lv', 'mk', 'ml', 'mr', 'ms', 'mzn', 'ne', 'new', 'nl', 'pa', 'pl', 'pnb', 'ps', 'pt', 'ro', 'ru', 'sd', 'sh', 'simple', 'sk', 'sq', 'sr', 'su', 'sv', 'ta', 'te', 'tg', 'th', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'wuu', 'xmf', 'yi', 'zh', 'zh-yue']\n",
      "Downloading Wiki network name: Laïcité in language: fr. Please note that this can take a while.\n",
      "Data succesfully saved. Wiki node name: Laïcité; downloaded in language: fr.\n",
      "The wikipedia page is available in the following languages:\n",
      "['af', 'ar', 'ary', 'arz', 'ast', 'az', 'be', 'be-x-old', 'bg', 'bh', 'bn', 'br', 'bs', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'diq', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'gl', 'he', 'hi', 'hr', 'hy', 'id', 'is', 'it', 'ja', 'ka', 'kn', 'ko', 'la', 'lad', 'lfn', 'li', 'lo', 'lv', 'mk', 'ml', 'mr', 'ms', 'mzn', 'ne', 'new', 'nl', 'pa', 'pl', 'pnb', 'ps', 'pt', 'ro', 'ru', 'sd', 'sh', 'simple', 'sk', 'sq', 'sr', 'su', 'sv', 'ta', 'te', 'tg', 'th', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'wuu', 'xmf', 'yi', 'zh', 'zh-yue']\n",
      "Downloading Wiki network name: Secularisme in language: nl. Please note that this can take a while.\n",
      "Data succesfully saved. Wiki node name: Secularisme; downloaded in language: nl.\n",
      "The wikipedia page is available in the following languages:\n",
      "['af', 'ar', 'ary', 'arz', 'ast', 'az', 'be', 'be-x-old', 'bg', 'bh', 'bn', 'br', 'bs', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'diq', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'fr', 'gl', 'he', 'hi', 'hr', 'hy', 'id', 'is', 'it', 'ja', 'ka', 'kn', 'ko', 'la', 'lad', 'lfn', 'li', 'lo', 'lv', 'mk', 'ml', 'mr', 'ms', 'mzn', 'ne', 'new', 'pa', 'pl', 'pnb', 'ps', 'pt', 'ro', 'ru', 'sd', 'sh', 'simple', 'sk', 'sq', 'sr', 'su', 'sv', 'ta', 'te', 'tg', 'th', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'wuu', 'xmf', 'yi', 'zh', 'zh-yue']\n",
      "Download of additional languages finished.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "test2.ID\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "selected_network = 'Oxford (en)'\n",
    "\n",
    "wiki_page_options = [v['AvailableLanguages'] for v in network_data.values() if v['title'] == all_networks[selected_network]['*'] if v['language'] == all_networks[selected_network]['lang']]\n",
    "language_options = [selected_network] + [k for k,v in all_networks.items() if {'lang': v['lang'], '*': v['*']} in wiki_page_options[0]]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "[{'label': i, 'value': i} for i in language_options] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'Oxford (en)', 'value': 'Oxford (en)'},\n",
       " {'label': 'Oxford (de)', 'value': 'Oxford (de)'},\n",
       " {'label': 'Oxford (fr)', 'value': 'Oxford (fr)'}]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "all_keys = set().union(*(d.keys() for d in language_options))\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b615f758eb2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-b615f758eb2c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "[{'label': a, 'value': a} for a in language_options] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': {'Oxford (en)': {'lang': 'en', '*': 'Oxford'}},\n",
       "  'value': {'Oxford (en)': {'lang': 'en', '*': 'Oxford'}}},\n",
       " {'label': ('Oxford (de)', {'lang': 'de', '*': 'Oxford'}),\n",
       "  'value': ('Oxford (de)', {'lang': 'de', '*': 'Oxford'})},\n",
       " {'label': ('Oxford (fr)', {'lang': 'fr', '*': 'Oxford'}),\n",
       "  'value': ('Oxford (fr)', {'lang': 'fr', '*': 'Oxford'})}]"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "[v for v in language_options] "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'Oxford (en)': {'lang': 'en', '*': 'Oxford'}},\n",
       " ('Oxford (de)', {'lang': 'de', '*': 'Oxford'}),\n",
       " ('Oxford (fr)', {'lang': 'fr', '*': 'Oxford'})]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def test_fun(input):\n",
    "    \n",
    "    text = WikiNode('Oxford', 'en')\n",
    "    if input > 1:\n",
    "        return text.node_ID\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "test_fun(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'22308'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_edge(0,1,weight=.1)\n",
    "G.add_edge(2,1,weight=.2)\n",
    "nx.write_gml(G,'g.gml')\n",
    "nx.write_graphml(G,'g.xml')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "G."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7ff998081d90>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}