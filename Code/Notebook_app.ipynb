{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "from flask import Flask, render_template, request\n",
    "import os\n",
    "import pygraphviz as pgv\n",
    "from pyvis.network import Network\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import collections \n",
    "import networkx as nx\n",
    "import requests\n",
    "import json\n",
    "# use dynetx for dynamic network visualization? -- when I can download and incoporate revision history? \n",
    "\n",
    "PATH = \"/home/teijehidde/Documents/Git Blog and Coding/Comparing Wikipedia Knowledge Networks (Network Analysis Page links)/Code/\"\n",
    "DATA_FILE = \"DATA.json\" \n",
    "\n",
    "# Loading JSON file: \n",
    "with open(PATH + DATA_FILE) as json_file:\n",
    "    network_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download data node_title from Wikimedia API and save to json file.  \n",
    "def downloadNetwork(node_title, language = \"en\"): \n",
    "\n",
    "    # setup and load existing data node.\n",
    "    API_ENDPOINT = \"https://\" + language + \".wikipedia.org/w/api.php\" # fr.wikipedia.org; https://en.wikipedia.org\n",
    "    wiki_data = []\n",
    "\n",
    "    # step 1: download data on the central (Ego) node of the network. \n",
    "    # setup API query and initial API call \n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": node_title,\n",
    "        \"prop\": \"links|info|langlinks\",\n",
    "        \"plnamespace\": 0, \n",
    "        \"pllimit\": 500,\n",
    "        \"lllimit\": 500, \n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "    wiki_data.append(response.json())\n",
    "    \n",
    "    # Continue API call until all data on ego node has been downloaded. \n",
    "    while 'continue' in wiki_data[-1].keys():\n",
    "        \n",
    "        PARAMS_CONT = PARAMS\n",
    "        PARAMS_CONT[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue']\n",
    "\n",
    "        response = S.get(url=API_ENDPOINT, params=PARAMS_CONT)\n",
    "        wiki_data.append(response.json())\n",
    "    \n",
    "    # step 2: use generator to download data on all additional nodes. \n",
    "    # setup API query for first generator API call (used to download data on all pages that are linked to node_title) \n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"generator\": \"links\",\n",
    "        \"titles\": node_title,\n",
    "        \"gplnamespace\": 0, \n",
    "        \"gpllimit\": 500, \n",
    "        \"plnamespace\": 0,\n",
    "        \"pllimit\": 500, \n",
    "        \"prop\": \"links\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "    wiki_data.append(response.json())\n",
    "\n",
    "    # Continue API call until all data on network is downloaded. \n",
    "    while 'continue' in wiki_data[-1].keys():\n",
    "\n",
    "        PARAMS_CONT = PARAMS\n",
    "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
    "            PARAMS_CONT[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
    "\n",
    "        if 'gplcontinue' in wiki_data[-1]['continue']: \n",
    "            PARAMS_CONT[\"gplcontinue\"] = wiki_data[-1]['continue']['gplcontinue']\n",
    "\n",
    "        response = S.get(url=API_ENDPOINT, params = PARAMS_CONT)\n",
    "        wiki_data.append(response.json())\n",
    "\n",
    "    # step 3: transforming data from API into unified dictionary. \n",
    "    # 3a: creating list of available nodes. \n",
    "    list_nodes = []\n",
    "\n",
    "    for item in wiki_data:\n",
    "        list_nodes = list_nodes + list(item['query']['pages'].keys())\n",
    "    list_nodes = list(set(list_nodes))\n",
    "\n",
    "    # 3b: Using list_nodes to go through raw data from API\n",
    "    for node in list_nodes:\n",
    "        node_data = {'node_ID': node, 'title': '', 'links': [], 'ego': [], 'language': language, 'AvailableLanguages': []}\n",
    "        \n",
    "        item_name = language + node\n",
    "        if item_name in network_data.keys():\n",
    "            node_data = network_data[item_name]\n",
    "        \n",
    "        for item in wiki_data:\n",
    "            if node in item['query']['pages'].keys(): \n",
    "                node_data['title'] = item['query']['pages'][node]['title']\n",
    "\n",
    "                if 'links' in item['query']['pages'][node].keys():\n",
    "                    for link in item['query']['pages'][node]['links']: \n",
    "                        node_data['links'].append(link['title'])\n",
    "\n",
    "                if 'langlinks' in item['query']['pages'][node].keys():\n",
    "                    node_data['AvailableLanguages'] = item['query']['pages'][node]['langlinks']\n",
    "\n",
    "                node_data['ego'].append(node_title)\n",
    "        \n",
    "        node_data['ego'] = list(set(node_data['ego']))\n",
    "        \n",
    "        network_data[language + node] = node_data\n",
    "\n",
    "    # Step 4: Saving data to json file. \n",
    "    try: \n",
    "        with open(PATH + DATA_FILE, 'w') as outfile:\n",
    "            json.dump(network_data, outfile)\n",
    "            print(\"Data succesfully saved. Wiki network name: \" + node_title + \"; downloaded in language: \" + language + \".\")\n",
    "\n",
    "    except: \n",
    "        print(\"Something went wrong. Check code.\")\n",
    "\n",
    "# optional, for debugging: \n",
    "#    finally:\n",
    "#        return wiki_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download additional languages of ego network.\n",
    "def downloadAdditionalLanguage(node_title, original_language = \"en\", requested_languages = []): # \"de\", \"fr\", \"nl\"\n",
    "\n",
    "    # download data fro JSON file. \n",
    "    with open(PATH + DATA_FILE) as json_file:\n",
    "        network_data = json.load(json_file)\n",
    "    \n",
    "    # make a list of the language that are available for requested page. \n",
    "    availabe_langs_titles = [v['AvailableLanguages'] for (k,v) in network_data.items() if v['title'] == node_title if v['language'] == original_language][0]\n",
    "    list_available_langs = []\n",
    "    for item in availabe_langs_titles: \n",
    "        list_available_langs.append(item['lang'])\n",
    "\n",
    "    # If no languages are requested, the function shows available languages. \n",
    "    if requested_languages == []:\n",
    "        print('The wikipedia page is available in the following languages:')         \n",
    "        print(list_available_langs)\n",
    "    \n",
    "    # Goes through avialble languages of a wikipedia page, and downloads those that were requested (using the downloadNetwork() function). \n",
    "    else:\n",
    "        for item in availabe_langs_titles: \n",
    "            if item['lang'] in requested_languages: \n",
    "                downloadNetwork(node_title = item['*'], language = item['lang'])\n",
    "    \n",
    "        print(\"Download of additional languages finished.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: provide titles of networks that are saved in the JSON file. Also provides the language they were saved in. \n",
    "def getNetworks(): \n",
    "\n",
    "    # download data fro JSON file. \n",
    "    with open(PATH + DATA_FILE) as json_file:\n",
    "        network_data = json.load(json_file)\n",
    "    \n",
    "    # make set of ego netowkrs and print \n",
    "    available_ego_networks = [(v['ego']) for (k,v) in network_data.items()]\n",
    "    available_ego_networks = set(list(chain(*available_ego_networks)))\n",
    "    \n",
    "    overview = {}\n",
    "    for item in available_ego_networks:\n",
    "        langs = []\n",
    "        nodes = [v for (k,v) in network_data.items() if v['title'] == item]\n",
    "        for node in nodes: \n",
    "            langs.append(node['language']) \n",
    "        overview[item] = langs\n",
    "    \n",
    "    print(overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate class Node. \n",
    "class WikiNode:\n",
    "    def __init__(self, node_title, language):\n",
    "        \n",
    "        # Select node in JSON file. \n",
    "        node_data = [v for (k,v) in network_data.items() if v['title'] == node_title if v['language'] == language][0]\n",
    "        \n",
    "        # Extract data and place in instance of Wikinode class. \n",
    "        self.node_title = node_data['title']\n",
    "        self.node_ID = node_data['node_ID']\n",
    "        self.node_links = node_data['links']\n",
    "        self.node_language = node_data['language']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate class WikiNetwork\n",
    "class WikiNetwork(WikiNode):\n",
    "   \n",
    "    def __init__(self,node_title,language):\n",
    "        # list available ego networks\n",
    "        available_ego_networks = [(v['ego']) for (k,v) in network_data.items()]\n",
    "        available_ego_networks = set(list(chain(*available_ego_networks)))\n",
    "        \n",
    "        # initiate network as class WikiNode, add additional attributes for class WikiNetwork \n",
    "        if node_title in available_ego_networks:\n",
    "            WikiNode.__init__(self, node_title, language)\n",
    "            # self.node_links = set([v['title'] for (k,v) in network_data.items() if v['ego'] == [node_title] if v['language'] == language])\n",
    "            self.network_nodes = []\n",
    "            self.network_edges = [] \n",
    "            self.network_status = []\n",
    "        \n",
    "            # Links are here used to build the network.\n",
    "            for link in self.node_links:\n",
    "                try:     \n",
    "                    Node2 = WikiNode(link, language)                \n",
    "                    purged_nodes = [x for x in Node2.node_links if x in self.node_links]\n",
    "                    purged_edges = []\n",
    "                    for purged_node in purged_nodes:\n",
    "                        purged_edges.append((link,purged_node))  \n",
    "                    self.network_nodes = self.network_nodes + purged_nodes\n",
    "                    self.network_edges = self.network_edges + purged_edges\n",
    "                except: \n",
    "                    print('Loading of node ' + link + ' failed.')\n",
    "                self.nodes_count = Counter(self.network_nodes)\n",
    "            print(\"Data Succesfully loaded.\")\n",
    "        \n",
    "        else: \n",
    "            print(\"Node not available. Download using downloadNetwork function.\") \n",
    "\n",
    "    def getStatusNetwork(self):\n",
    "        print('WIP')\n",
    "\n",
    "    def getNodesEdges(self,threshold):\n",
    "        selected_nodes = [k for k,v in self.nodes_count.items() if float(v) >= threshold]\n",
    "        selected_edges = [(a,b) for a,b in self.network_edges if a in selected_nodes and b in selected_nodes]\n",
    "\n",
    "        nodes_network = []\n",
    "        for node in selected_nodes:\n",
    "            node_tuple = (node, {\"name\": node})\n",
    "            nodes_network.append(node_tuple)\n",
    "\n",
    "        return (nodes_network,selected_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(WikiNodesEdges):\n",
    "    \n",
    "    Graph = nx.Graph()\n",
    "\n",
    "    Graph.add_nodes_from(WikiNodesEdges[0])\n",
    "    Graph.add_edges_from(WikiNodesEdges[1])\n",
    "\n",
    "    netdraw = Network('2000px', '2000px')\n",
    "    netdraw.from_nx(Graph)\n",
    "    netdraw.barnes_hut()\n",
    "\n",
    "    title = \"test.html\"\n",
    "    netdraw.show(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "########################################\n",
    "# FROM HERE RUN TIME STARTS # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data succesfully saved. Wiki network name: Secularism; downloaded in language: en.\n"
     ]
    }
   ],
   "source": [
    "downloadNetwork('Secularism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The wikipedia page is available in the following languages:\n['af', 'ar', 'ary', 'arz', 'ast', 'az', 'be', 'be-x-old', 'bg', 'bh', 'bn', 'br', 'bs', 'ca', 'ckb', 'cs', 'cy', 'da', 'de', 'diq', 'el', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'fr', 'gl', 'he', 'hi', 'hr', 'hy', 'id', 'is', 'it', 'ja', 'ka', 'kn', 'ko', 'la', 'lad', 'lfn', 'li', 'lo', 'lv', 'mk', 'ml', 'mr', 'ms', 'mzn', 'ne', 'new', 'nl', 'pa', 'pl', 'pnb', 'pt', 'ro', 'ru', 'sd', 'sh', 'simple', 'sk', 'sq', 'sr', 'su', 'sv', 'ta', 'te', 'tg', 'th', 'tr', 'tt', 'uk', 'ur', 'uz', 'vi', 'wuu', 'xmf', 'yi', 'zh', 'zh-yue']\nDownload of additional languages finished.\n"
     ]
    }
   ],
   "source": [
    "downloadAdditionalLanguage(node_title = 'Secularism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data succesfully saved. Wiki network name: علمانية; downloaded in language: ar.\n",
      "Data succesfully saved. Wiki network name: Säkularismus; downloaded in language: de.\n",
      "Data succesfully saved. Wiki network name: Laïcité; downloaded in language: fr.\n",
      "Data succesfully saved. Wiki network name: Secularisme; downloaded in language: nl.\n",
      "Download of additional languages finished.\n"
     ]
    }
   ],
   "source": [
    "downloadAdditionalLanguage(node_title = 'Secularism', original_language = \"en\", requested_languages = [\"de\", \"fr\", \"ar\", \"nl\"]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Secularism': ['en'], 'Terrorism': ['en'], 'Religion': ['de', 'fr', 'en'], 'Terrorismus': ['de'], 'Säkularismus': ['de'], 'England': ['en'], 'دين (معتقد)': ['ar'], 'علمانية': ['ar'], 'Terrorisme': ['fr'], 'Religie': ['nl'], 'إرهاب': ['ar'], 'Laïcité': ['fr'], 'Secularisme': ['nl']}\n"
     ]
    }
   ],
   "source": [
    "getNetworks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Succesfully loaded.\n",
      "Data Succesfully loaded.\n",
      "Loading of node Church of Scotland Act 1921 failed.\n",
      "Loading of node Critique de la Scientologie failed.\n",
      "Loading of node Gennade II Scholarius failed.\n",
      "Data Succesfully loaded.\n",
      "Data Succesfully loaded.\n",
      "Data Succesfully loaded.\n"
     ]
    }
   ],
   "source": [
    "Networks = []\n",
    "Networks.append(WikiNetwork('Secularism', language='en'))\n",
    "Networks.append(WikiNetwork('Säkularismus', language='de'))\n",
    "Networks.append(WikiNetwork('Laïcité', language='fr'))\n",
    "Networks.append(WikiNetwork('علمانية', language='ar'))\n",
    "Networks.append(WikiNetwork('Secularisme', language='nl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph = []\n",
    "\n",
    "Graph.append(Networks[0].getNodesEdges(15))\n",
    "Graph.append(Networks[1].getNodesEdges(2))\n",
    "Graph.append(Networks[2].getNodesEdges(5))\n",
    "Graph.append(Networks[3].getNodesEdges(2))\n",
    "Graph.append(Networks[4].getNodesEdges(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "drawGraph(Graph[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "source": [
    "len(Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}