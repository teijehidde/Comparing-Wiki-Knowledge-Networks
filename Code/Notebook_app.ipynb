{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Setup packages\n",
    "import os\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import collections \n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# all these network algorithms are currently being tried out. \n",
    "import networkx as nx\n",
    "from networkx.algorithms import approximation\n",
    "from networkx.algorithms import community\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "from networkx.utils import not_implemented_for \n",
    "__all__ = [\n",
    "    \"eccentricity\",\n",
    "    \"diameter\",\n",
    "    \"radius\",\n",
    "    \"periphery\",\n",
    "    \"center\",\n",
    "    \"barycenter\",\n",
    "    \"degree_centrality\",\n",
    "    \"constraint\", \n",
    "    \"local_constraint\", \n",
    "    \"effective_size\"\n",
    "]\n",
    "\n",
    "\n",
    "# For network visualization: \n",
    "from pyvis.network import Network\n",
    "# use dynetx for dynamic network visualization? -- when I can download and incoporate revision history? \n",
    "\n",
    "PATH = \"/home/teijehidde/Documents/Git Blog and Coding/data_dump/\"\n",
    "DATA_FILE = \"DATA.json\" \n",
    "\n",
    "# Loading JSON file: \n",
    "with open(PATH + DATA_FILE) as json_file:\n",
    "    network_data = json.load(json_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def SaveData(wiki_data, node_title, lang):\n",
    "    # step 1: transforming data from API into unified dictionary. \n",
    "    # 1a: creating list of available nodes. \n",
    "    all_nodes = []\n",
    "\n",
    "    for item in wiki_data:\n",
    "        all_nodes = all_nodes + list(item['query']['pages'].keys())\n",
    "    all_nodes = list(set(all_nodes))\n",
    "\n",
    "    # 1b: Using all_nodes to go through raw data from API -- in this case this should just by 1 node. \n",
    "    for node in all_nodes:\n",
    "        node_data = {'node_ID': node, 'title': '', 'links': [], 'ego': [], 'language': lang, 'AvailableLanguages': []}\n",
    "        \n",
    "        item_name = lang + node\n",
    "        if item_name in network_data.keys():\n",
    "            node_data = network_data[item_name]\n",
    "        \n",
    "        for item in wiki_data:\n",
    "            if node in item['query']['pages'].keys(): \n",
    "                node_data['title'] = item['query']['pages'][node]['title']\n",
    "\n",
    "                if 'links' in item['query']['pages'][node].keys():\n",
    "                    for link in item['query']['pages'][node]['links']: \n",
    "                        node_data['links'].append(link['title'])\n",
    "\n",
    "                if 'langlinks' in item['query']['pages'][node].keys():\n",
    "                    node_data['AvailableLanguages'] = item['query']['pages'][node]['langlinks']\n",
    "\n",
    "                node_data['ego'].append(node_title)\n",
    "        \n",
    "        node_data['ego'] = list(set(node_data['ego']))\n",
    "        \n",
    "        network_data[lang + node] = node_data\n",
    "\n",
    "    # Step 2: Saving data to json file. \n",
    "    try: \n",
    "        with open(PATH + DATA_FILE, 'w') as outfile:\n",
    "            json.dump(network_data, outfile)\n",
    "            print(\"Data succesfully saved. Wiki node name: \" + node_title + \"; downloaded in language: \" + lang + \".\")\n",
    "\n",
    "    except: \n",
    "        print(\"Something went wrong. Check code.\")\n",
    "\n",
    "# optional, for debugging: \n",
    "#   finally:\n",
    "#       return wiki_data\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Function: download additional languages of ego network.\n",
    "def downloadAdditionalLanguage(node_title, original_lang, additional_langs = []): # \"de\", \"fr\", \"nl\"\n",
    "\n",
    "    # download data fro JSON file. \n",
    "    with open(PATH + DATA_FILE) as json_file:\n",
    "        network_data = json.load(json_file)\n",
    "    \n",
    "    # make a list of the language that are available for requested page. \n",
    "    available_languages = [v['AvailableLanguages'] for (k,v) in network_data.items() if v['title'] == node_title if v['language'] == original_lang][0]\n",
    "    list_available_languages = []\n",
    "    for item in available_languages: \n",
    "         list_available_languages.append(item['lang'])\n",
    "\n",
    "    # If no languages are requested, the function shows available languages. \n",
    "    if additional_langs == []:\n",
    "        pass\n",
    "\n",
    "    if additional_langs == [\"available_langs\"]:\n",
    "        print('The wikipedia page is available in the following languages:')         \n",
    "        print(list_available_languages)\n",
    "    \n",
    "    # Goes through avialble languages of a wikipedia page, and downloads those that were requested (using the downloadNetwork() function). \n",
    "    else:\n",
    "        for item in available_languages: \n",
    "            if item['lang'] in additional_langs:\n",
    "                downloadNetworks(node_title = item['*'], original_lang = item['lang'], additional_langs = [])\n",
    "    \n",
    "        print(\"Download of additional languages finished.\") \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Function: download data on links from ALL PAGES linked to 'node_title' (but excluding node_title itself!) from Wikimedia API and save to json file.  \n",
    "def downloadNetworks(node_title, original_lang = \"en\", additional_langs = [\"ar\" \"de\", \"fr\", \"nl\"]): \n",
    "\n",
    "    # setup and load existing data node.\n",
    "    API_ENDPOINT = \"https://\" + original_lang + \".wikipedia.org/w/api.php\" # fr.wikipedia.org; https://en.wikipedia.org\n",
    "    wiki_data = []\n",
    "\n",
    "    # step 1: download data on the central node of the network (incl. available languages). \n",
    "    # setup API query and initial API call \n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": node_title,\n",
    "        \"prop\": \"links|info|langlinks\",\n",
    "        \"plnamespace\": 0, \n",
    "        \"pllimit\": 500,\n",
    "        \"lllimit\": 500, \n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "    wiki_data.append(response.json())\n",
    "    \n",
    "    # Continue API call until all data on ego node has been downloaded. \n",
    "    while 'continue' in wiki_data[-1].keys():\n",
    "        \n",
    "        PARAMS_CONT = PARAMS\n",
    "        PARAMS_CONT[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue']\n",
    "\n",
    "        response = S.get(url=API_ENDPOINT, params=PARAMS_CONT)\n",
    "        wiki_data.append(response.json())\n",
    "\n",
    "    # step 2: use generator to download data on all additional nodes. \n",
    "    # setup API query for first generator API call (used to download data on all pages that are linked to node_title) \n",
    "    print(\"Downloading Wiki network name: \" + node_title + \" in language: \" + original_lang + \". Please note that this can take a while.\")\n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"generator\": \"links\",\n",
    "        \"titles\": node_title,\n",
    "        \"gplnamespace\": 0, \n",
    "        \"gpllimit\": 500, \n",
    "        \"plnamespace\": 0,\n",
    "        \"pllimit\": 500, \n",
    "        \"prop\": \"links\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "    wiki_data.append(response.json())\n",
    "\n",
    "    # Continue API call until all data on network is downloaded. \n",
    "    while 'continue' in wiki_data[-1].keys():\n",
    "\n",
    "        PARAMS_CONT = PARAMS\n",
    "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
    "            PARAMS_CONT[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
    "\n",
    "        if 'gplcontinue' in wiki_data[-1]['continue']: \n",
    "            PARAMS_CONT[\"gplcontinue\"] = wiki_data[-1]['continue']['gplcontinue']\n",
    "\n",
    "        response = S.get(url=API_ENDPOINT, params = PARAMS_CONT)\n",
    "        wiki_data.append(response.json())\n",
    "\n",
    "    # step 3: transform and save data:  \n",
    "    SaveData(wiki_data, node_title=node_title, lang=original_lang)\n",
    "\n",
    "    # step 4: download additional languages: \n",
    "    downloadAdditionalLanguage(node_title = node_title, original_lang = original_lang, additional_langs = additional_langs)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# function: provide titles of networks that are saved in the JSON file. Also provides the language they were saved in. \n",
    "def getDownloadedNetworks(): \n",
    "\n",
    "    # download data from JSON file. \n",
    "    with open(PATH + DATA_FILE) as json_file:\n",
    "        network_data = json.load(json_file)\n",
    "    \n",
    "    # create set of ego network names.  \n",
    "    downloaded_networks = [(v['ego']) for (k,v) in network_data.items()]\n",
    "    downloaded_networks = set(list(chain(*downloaded_networks)))\n",
    "    downloaded_networks = [v for (k,v) in network_data.items() if v['title'] in downloaded_networks]\n",
    "\n",
    "    # print names of ego networks and language that they have been downloaded in. \n",
    "    items = {}  \n",
    "    for network in downloaded_networks: \n",
    "        items[network['title'] + ' (' + network['language'] + ')'] = {'lang':  network['language'], '*': network['title']}    \n",
    "    return(items)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def degree_centrality(G):\n",
    "# copy-pasted from: https://networkx.org/documentation/stable/_modules/networkx/algorithms/centrality/degree_alg.html#degree_centrality\n",
    "    \n",
    "    if len(G) <= 1:\n",
    "        return {n: 1 for n in G}\n",
    "\n",
    "    s = 1.0 / (len(G) - 1.0)\n",
    "    centrality = {n: d * s for n, d in G.degree()}\n",
    "    return centrality\n",
    "\n",
    "def eccentricity(G, v=None, sp=None):\n",
    "# copy-pasted from: https://networkx.org/documentation/stable/_modules/networkx/algorithms/distance_measures.html#eccentricity\n",
    "\n",
    "    order = G.order()\n",
    "\n",
    "    e = {}\n",
    "    for n in G.nbunch_iter(v):\n",
    "        if sp is None:\n",
    "            length = nx.single_source_shortest_path_length(G, n)\n",
    "            L = len(length)\n",
    "        else:\n",
    "            try:\n",
    "                length = sp[n]\n",
    "                L = len(length)\n",
    "            except TypeError as e:\n",
    "                raise nx.NetworkXError('Format of \"sp\" is invalid.') from e\n",
    "        if L != order:\n",
    "            if G.is_directed():\n",
    "                msg = (\n",
    "                    \"Found infinite path length because the digraph is not\"\n",
    "                    \" strongly connected\"\n",
    "                )\n",
    "            else:\n",
    "                msg = \"Found infinite path length because the graph is not\" \" connected\"\n",
    "            raise nx.NetworkXError(msg)\n",
    "\n",
    "        e[n] = max(length.values())\n",
    "\n",
    "    if v in G:\n",
    "        return e[v]  # return single value\n",
    "    else:\n",
    "        return e\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# Initiate class Node. \n",
    "class WikiNode:\n",
    "    def __init__(self, node_title, lang):\n",
    "        \n",
    "        # Select node in JSON file (by title and language). \n",
    "        node_data = [v for (k,v) in network_data.items() if v['title'] == node_title if v['language'] == lang][0]\n",
    "        \n",
    "        # Extract data and place in instance of Wikinode class. \n",
    "        self.node_title = node_data['title']\n",
    "        self.node_ID = node_data['node_ID']\n",
    "        self.node_links = node_data['links']\n",
    "        self.node_lang = node_data['language']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Initiate class WikiNetwork\n",
    "class WikiNetwork(WikiNode):\n",
    "   \n",
    "    def __init__(self,node_title, lang):\n",
    "        \n",
    "        # initiate the central node of the network as class WikiNode, add additional attributes for class WikiNetwork \n",
    "        WikiNode.__init__(self, node_title, lang)\n",
    "        self.network_nodes = {}\n",
    "        self.network_links = []\n",
    "        self.network_edges = [] \n",
    "        self.network_status = []\n",
    "    \n",
    "        # Go through node_links of the central node (node_title) to build network.\n",
    "        for link in self.node_links:\n",
    "            try:     \n",
    "                Node2 = WikiNode(link, lang)             \n",
    "                purged_links = [x for x in Node2.node_links if x in self.node_links]\n",
    "                purged_edges = []\n",
    "                for purged_link in purged_links:\n",
    "                    purged_edges.append((link,purged_link))  \n",
    "                self.network_nodes[Node2.node_ID] = Node2\n",
    "                self.network_links = self.network_links + purged_links\n",
    "                self.network_edges = self.network_edges + purged_edges\n",
    "            except: \n",
    "                print('Loading of node ' + link + ' failed.')\n",
    "            self.links_count = Counter(self.network_links)\n",
    "\n",
    "    def getNodes(self, type=\"cytoscape\", threshold=0):\n",
    "        selected_nodes = [k for k,v in self.links_count.items() if float(v) >= threshold]\n",
    "        \n",
    "        if type == 'networkx':\n",
    "            return [(i, {\"name\": i}) for i in selected_nodes]\n",
    "\n",
    "        if type == 'cytoscape':\n",
    "            return [{'data': {'id': i, \"label\": i}} for i in selected_nodes]\n",
    "\n",
    "    def getEdges(self,type=\"cytoscape\", threshold=0):  \n",
    "        selected_nodes = [k for k,v in self.links_count.items() if float(v) >= threshold]\n",
    "        edges_network = [(a,b) for a,b in self.network_edges if a in selected_nodes and b in selected_nodes]\n",
    "\n",
    "        if type == 'networkx':\n",
    "            return edges_network\n",
    "\n",
    "        if type == 'cytoscape':\n",
    "            return [{'data': {'source': a, \"target\": b}} for a,b in edges_network]\n",
    "    \n",
    "    def getCommunities(self,threshold=0):  \n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(self.getEdges(type = 'networkx', threshold= threshold))\n",
    "        return greedy_modularity_communities(G)\n",
    "\n",
    "    def getStatsNodes(self,threshold=0):\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(self.getEdges(type = 'networkx', threshold= threshold))\n",
    "\n",
    "        data = {}\n",
    "        degree_centrality_nodes = degree_centrality(G)\n",
    "        eccentricity_nodes = eccentricity(G)\n",
    "\n",
    "        for item in G.nodes: \n",
    "            data[item] = {'Centrality': degree_centrality_nodes[item], 'Eccentricity': eccentricity_nodes[item]} \n",
    "\n",
    "        return(data)\n",
    "    \n",
    "    def getStatsCommunities(self, node):\n",
    "        # TODO: return an numpy array with stats per node: \n",
    "        # Algorithms to consider (see networkx):\n",
    "        # - Distance measures: barycenter, center, [ALSO APPLY TO COMMUNITIES?]\n",
    "        # - dominating_set(G, start_with=None) [ALSO APPLY TO COMMUNITIES?]\n",
    "        # - Group Centrality\n",
    "        # - ... \n",
    "        # if nodes == None: \n",
    "          #  node = self.node_links\n",
    "\n",
    "        print('WIP')\n",
    "    \n",
    "    def getStatsNetwork(self): \n",
    "        return(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"A\": self.node_ID,\n",
    "                    \"B\": pd.Timestamp(\"20200102\"),\n",
    "                    \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\n",
    "                    \"D\": np.array([3] * 4, dtype=\"int32\"),\n",
    "                    \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n",
    "                    \"F\": self.node_title,\n",
    "                }\n",
    "                )\n",
    "            )   \n",
    "        # TODO: return a dictionary with stats on network: \n",
    "        # Algorithms to consider (see networkx):\n",
    "        # - number nodes, number edges, average edges per node. \n",
    "        # - Distance measures: barycenter, center, [ALSO APPLY TO COMMUNITIES?]\n",
    "        # - dominating_set(G, start_with=None) [ALSO APPLY TO COMMUNITIES?]\n",
    "        # - node_connectivity\n",
    "        # - k_components\n",
    "        # - average_clustering\n",
    "        # - Small-world\n",
    "        # - Summarization [NB - possibly use to improve render time graph visualizations.]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Class: a collection of wiki networks of the same topic, in different languages. Automatically takes all languages that have been downloaded before. \n",
    "class WikiNetworkCollection():\n",
    "\n",
    "        def __init__(self,chosen_networks):\n",
    "        \n",
    "                # make list available ego networks\n",
    "                all_downloaded_networks = getDownloadedNetworks()\n",
    "                available_languages = [v['AvailableLanguages'] for (k,v) in network_data.items() if v['title'] == node_title][0]\n",
    "                \n",
    "                topic_networks = []\n",
    "                topic_networks = [{'lang': original_language, '*': node_title}] + [v for v in available_languages if v in all_downloaded_networks]\n",
    "\n",
    "                # initiate tclass WikiNetwork for each available language.  \n",
    "                self.networks = {}\n",
    "                if topic_networks is not []:\n",
    "                        for network in topic_networks:\n",
    "                                self.networks[network['lang'] + '_' + network['*']] = WikiNetwork(node_title = network['*'], language = network['lang'])\n",
    "\n",
    "        def getStatsIsomorphism(self):\n",
    "        # TODO: return an numpy array with stats per network: \n",
    "        # - ... related to similarities / difference of network to other networks in collection. \n",
    "                print('WIP')\n",
    "        # Algorithms to consider: \n",
    "        # - networkx: isomorphism. \n",
    "        # - networkx: Similarity Measures\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "########################################\n",
    "########################################\n",
    "# FROM HERE RUN TIME STARTS # "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "getDownloadedNetworks()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Flask (en)': {'lang': 'en', '*': 'Flask'},\n",
       " 'Kolba (de)': {'lang': 'de', '*': 'Kolba'},\n",
       " 'Royston (en)': {'lang': 'en', '*': 'Royston'},\n",
       " 'Oxford (en)': {'lang': 'en', '*': 'Oxford'},\n",
       " 'Flask (fr)': {'lang': 'fr', '*': 'Flask'},\n",
       " 'Oxford (de)': {'lang': 'de', '*': 'Oxford'},\n",
       " 'Oxford (fr)': {'lang': 'fr', '*': 'Oxford'},\n",
       " 'Vaccine (en)': {'lang': 'en', '*': 'Vaccine'},\n",
       " 'Vaccine (fr)': {'lang': 'fr', '*': 'Vaccine'},\n",
       " 'Vaccin (fr)': {'lang': 'fr', '*': 'Vaccin'},\n",
       " 'Vaccine (de)': {'lang': 'de', '*': 'Vaccine'},\n",
       " 'Impfstoff (de)': {'lang': 'de', '*': 'Impfstoff'},\n",
       " 'Secularism (en)': {'lang': 'en', '*': 'Secularism'},\n",
       " 'علمانية (ar)': {'lang': 'ar', '*': 'علمانية'},\n",
       " 'Laïcité (fr)': {'lang': 'fr', '*': 'Laïcité'},\n",
       " 'Secularisme (nl)': {'lang': 'nl', '*': 'Secularisme'},\n",
       " 'لقاح (ar)': {'lang': 'ar', '*': 'لقاح'},\n",
       " 'Aşı (tıp) (tr)': {'lang': 'tr', '*': 'Aşı (tıp)'}}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# downloadNetworks(\"Vaccine\", original_lang='en', additional_langs=['de', 'ar', 'fr', 'tr'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "source": [
    "with open(PATH + DATA_FILE) as json_file:\n",
    "    network_data = json.load(json_file)\n",
    "value = 'Oxford (en)'\n",
    "all_networks = getDownloadedNetworks()\n",
    "\n",
    "wiki_page = WikiNetwork(node_title=all_networks[value]['*'], lang=all_networks[value]['lang'] )\n",
    "list_colours = ['red', 'blue', 'purple','orange','green','olive', 'maroon', 'brown','lime','teal' ]\n",
    "\n",
    "nodes = wiki_page.getNodes(type='cytoscape', threshold=0)\n",
    "edges = wiki_page.getEdges(type='cytoscape', threshold=0)\n",
    "communities = wiki_page.getCommunities()\n",
    "stats_nodes = wiki_page.getStatsNodes(threshold=0)\n",
    "\n",
    "d = {'selector': [ ],\n",
    "      'style':   [ ] } \n",
    "\n",
    "for item in stats_nodes: \n",
    "    d['selector'].append(item)\n",
    "    d['style'].append({ 'background-opacity': stats_nodes[item]['Centrality'], \n",
    "                        'width': stats_nodes[item]['Centrality'], \n",
    "                        'height': stats_nodes[item]['Centrality']\n",
    "                    })\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading of node C. S. Lewis Nature Reserve failed.\n",
      "Loading of node Labstep failed.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "source": [
    "my_stylesheet = pd.DataFrame(data=d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "source": [
    "my_stylesheet"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                             selector  \\\n",
       "0                      102 Dalmatians   \n",
       "1                     Wayback Machine   \n",
       "2          2011 United Kingdom census   \n",
       "3                          Birmingham   \n",
       "4                       British Asian   \n",
       "..                                ...   \n",
       "801     List of attractions in Oxford   \n",
       "802                  Oxford Town Hall   \n",
       "803                          Ramallah   \n",
       "804                Tourist attraction   \n",
       "805  Vivian Smith, 1st Baron Bicester   \n",
       "\n",
       "                                             style  \n",
       "0    {'background-opacity': 0.0012422360248447205}  \n",
       "1                      {'background-opacity': 0.2}  \n",
       "2      {'background-opacity': 0.03850931677018634}  \n",
       "3       {'background-opacity': 0.1863354037267081}  \n",
       "4      {'background-opacity': 0.03229813664596273}  \n",
       "..                                             ...  \n",
       "801  {'background-opacity': 0.0012422360248447205}  \n",
       "802  {'background-opacity': 0.0012422360248447205}  \n",
       "803  {'background-opacity': 0.0012422360248447205}  \n",
       "804  {'background-opacity': 0.0012422360248447205}  \n",
       "805  {'background-opacity': 0.0012422360248447205}  \n",
       "\n",
       "[806 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selector</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102 Dalmatians</td>\n",
       "      <td>{'background-opacity': 0.0012422360248447205}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wayback Machine</td>\n",
       "      <td>{'background-opacity': 0.2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011 United Kingdom census</td>\n",
       "      <td>{'background-opacity': 0.03850931677018634}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Birmingham</td>\n",
       "      <td>{'background-opacity': 0.1863354037267081}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>British Asian</td>\n",
       "      <td>{'background-opacity': 0.03229813664596273}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>List of attractions in Oxford</td>\n",
       "      <td>{'background-opacity': 0.0012422360248447205}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Oxford Town Hall</td>\n",
       "      <td>{'background-opacity': 0.0012422360248447205}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>Ramallah</td>\n",
       "      <td>{'background-opacity': 0.0012422360248447205}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Tourist attraction</td>\n",
       "      <td>{'background-opacity': 0.0012422360248447205}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Vivian Smith, 1st Baron Bicester</td>\n",
       "      <td>{'background-opacity': 0.0012422360248447205}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>806 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}