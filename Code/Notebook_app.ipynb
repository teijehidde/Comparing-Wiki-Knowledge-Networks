{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\" \\nNB: NEXT STEPS \\n\\n1: Create a WikiGraph class, based on networkx nx.Graph instance. It should have method to call status grpah (no. of nodes, edges, etc; analysis on individual nodes and comeplete graph; and include visualisation of graph. It should also be possible to export csv data on graph. All this should be build on networkx, pyvis and numpy. \\n2: Revise API call to act at network (not node) level, and based completely on generators. -- see website from wikipedia. Also consider implementing zipping. \\n3: Restructure (again) how data is saved. a) The 'plcontinue value' should NOT be linked to node (similar to how wikimedia treats it). b) Implement timestamp c) save the wikipedia version (english, french, arabic) that data is from. \\n4: Start playting around with comparing knowledge networks around topics _in different languages_. Cross case comparison on same topic. \\n5: Start playting with Django for building interacting website to visualize descriptive analysis. On the face of it Django seems more straighforward than Flask. \\n6: -- later -- incorporate 'revision' in data that is downloaded. \\n7: -- later -- work with dynamic network visualisations and analysis. They do exist in Python, but do not know how stable / efficient they are yet.  \\n8: While doing this, do not forget to push to git & keep up adding notes to code. Also for myself.. \\n\""
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "\"\"\" \n",
    "NB: NEXT STEPS \n",
    "\n",
    "1: Create a WikiGraph class, based on networkx nx.Graph instance. It should have method to call status grpah (no. of nodes, edges, etc; analysis on individual nodes and comeplete graph; and include visualisation of graph. It should also be possible to export csv data on graph. All this should be build on networkx, pyvis and numpy. \n",
    "2: Revise API call to act at network (not node) level, and based completely on generators. -- see website from wikipedia. Also consider implementing zipping. \n",
    "3: Restructure (again) how data is saved. a) The 'plcontinue value' should NOT be linked to node (similar to how wikimedia treats it). b) Implement timestamp c) save the wikipedia version (english, french, arabic) that data is from. \n",
    "4: Start playting around with comparing knowledge networks around topics _in different languages_. Cross case comparison on same topic. \n",
    "5: Start playting with Django for building interacting website to visualize descriptive analysis. On the face of it Django seems more straighforward than Flask. \n",
    "6: -- later -- incorporate 'revision' in data that is downloaded. \n",
    "7: -- later -- work with dynamic network visualisations and analysis. They do exist in Python, but do not know how stable / efficient they are yet.  \n",
    "8: While doing this, do not forget to push to git & keep up adding notes to code. Also for myself.. \n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup \n",
    "from flask import Flask, render_template, request\n",
    "import os\n",
    "import pygraphviz as pgv\n",
    "from pyvis.network import Network\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import requests\n",
    "import json\n",
    "# use dynetx for dynamic network visualization? -- when I can download and incoporate revision history? \n",
    "\n",
    "PATH = \"/home/teijehidde/Documents/Git Blog and Coding/Project one (wikipedia SNA)/Code/\"\n",
    "DATA_FILE = \"networkdataEN.json\" # networkdataFR.json; networkdataEN.json\n",
    "WIKI_URL = \"https://en.wikipedia.org\" # fr.wikipedia.org; https://en.wikipedia.org\n",
    "API_ENDPOINT = WIKI_URL + \"/w/api.php\"\n",
    "LIMIT_LINKS_PER_NODE = 500\n",
    "LIMIT_API_REQUESTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading previously saved link data.\n",
    "with open(PATH + DATA_FILE) as json_file:\n",
    "    network_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: download data node_title from Wikimedia API, add to network_data runtime object and save to json file.  \n",
    "# NB: implement. Use of | to ask multiple pages at same time! Does this work with plcontinue?? See https://www.mediawiki.org/wiki/API:Etiquette \n",
    "def downloadNode(node_title, continue_pageid = None):\n",
    "\n",
    "    # setup and load existing data node.\n",
    "    links_wiki = []\n",
    "    if node_title in network_data.keys(): \n",
    "        if network_data[node_title]['status'] == 'incomplete':\n",
    "            links_wiki = links_wiki + network_data[node_title]['links']\n",
    "\n",
    "    # requesting data via wikimedia API.  \n",
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": node_title,\n",
    "        \"prop\": \"links\",\n",
    "        \"plcontinue\": continue_pageid, #NB: THIS SHOULD BE MAX PAGEID of result, then call AGAIN... \n",
    "        \"plnamespace\": 0, # only load wikipedia main/articles. \n",
    "        \"pllimit\": 500 # can go up to 500. Go for max? \n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)\n",
    "\n",
    "    # Transforming dat response + error handling.  \n",
    "    data_wiki = response.json()\n",
    "    node = next(iter(data_wiki['query']['pages']))\n",
    "    \n",
    "    try: \n",
    "        for x in data_wiki['query']['pages'][node]['links']:\n",
    "            links_wiki.append(x['title'])\n",
    "                \n",
    "        node_data = {'status': 'complete', 'node_ID': node, 'links': links_wiki, 'timestamp': 'TODO', 'ego': 0} # , 'revisions': 'TODO'\n",
    "        if 'continue' in data_wiki.keys(): \n",
    "            node_data['status'] = 'incomplete' \n",
    "            node_data['plcontinue'] = data_wiki['continue']['plcontinue']\n",
    "        network_data[node_title] = node_data\n",
    "        return network_data[node_title]\n",
    "        \n",
    "    except:\n",
    "        node_data = {'status': 'dead', 'timestamp': 'TODO'}\n",
    "        network_data[node_title] = node_data\n",
    "        return network_data[node_title]\n",
    "    \n",
    "    finally:   \n",
    "        with open(PATH + DATA_FILE, 'w') as outfile:\n",
    "            json.dump(network_data, outfile)\n",
    "            print(\"Data succesfully saved. Wikipage name: \" + node_title + \". Status: \" + network_data[node_title]['status'] + \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate class Node. \n",
    "class WikiNode:\n",
    "    def __init__(self,node_title):\n",
    "        self.node_title = node_title\n",
    "        self.node_status = 'empty'\n",
    "        self.node_links = []\n",
    "        \n",
    "        # here the json file is read. If the data on wikipedia links is not available, it returns the node as 'empty'. \n",
    "        if node_title in network_data.keys():\n",
    "            self.node_status = network_data[node_title]['status']\n",
    "        else: self.node_status = 'empty'\n",
    "        \n",
    "        # if there is data on wikipedia links available (if the node is not a deadlink, thus 'alive') links are added to the instance of WikiNode. \n",
    "        if self.node_status == 'complete':\n",
    "            self.node_links = network_data[node_title]['links']\n",
    "\n",
    "    # In case data is not available, this method calls a function to call the wikimedia API to download data from wikimedia. \n",
    "    def downloadNode(self,continue_pageid=None,):\n",
    "        \n",
    "        if self.node_title not in network_data.keys(): \n",
    "            downloadNode(self.node_title)\n",
    "        while network_data[self.node_title]['status'] == 'incomplete' or network_data[self.node_title]['status'] == 'empty':  \n",
    "            if 'plcontinue' in network_data[self.node_title].keys(): \n",
    "                x = network_data[self.node_title]['plcontinue']\n",
    "            else: x = None  \n",
    "            downloadNode(self.node_title, continue_pageid = x)\n",
    "\n",
    "        self.__init__(self.node_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate class WikiNetwork\n",
    "class WikiNetwork(WikiNode):\n",
    "    \n",
    "    def __init__(self,node_title):\n",
    "        # initiate network as class WikiNode, add additional attributes for class WikiNetwork \n",
    "        WikiNode.__init__(self, node_title)\n",
    "        self.network_nodes = []\n",
    "        self.network_edges = [] \n",
    "        self.network_status = []\n",
    "        \n",
    "        # if the central node has links (if it is 'alive' and not a deadlink on wikipedia) then the links are here used to build the network.  \n",
    "        if self.node_status == 'complete':\n",
    "            self.node_links.append(node_title)\n",
    "\n",
    "            for link in self.node_links:\n",
    "                Node2 = WikiNode(link)\n",
    "                self.network_status  = self.network_status + [Node2.node_status] \n",
    "                if Node2.node_status == 'complete':\n",
    "                    purged_nodes = [x for x in Node2.node_links if x in self.node_links]\n",
    "                    purged_edges = []\n",
    "                    for purged_node in purged_nodes:\n",
    "                        purged_edges.append((link,purged_node))  \n",
    "                    self.network_nodes = self.network_nodes + purged_nodes\n",
    "                    self.network_edges = self.network_edges + purged_edges                             \n",
    "            self.nodes_count = Counter(self.network_nodes)\n",
    "            print(\"Data Succesfully loaded.\")\n",
    "\n",
    "\n",
    "    def getStatusNetwork(self):     \n",
    "        return Counter(self.network_status)\n",
    "\n",
    "\n",
    "    def downloadNetwork(self,callLimit): \n",
    "        if self.node_status != 'complete': \n",
    "            self.downloadNode()\n",
    "\n",
    "        call = 0\n",
    "        for link in self.network_nodes:\n",
    "            Node2 = WikiNode(link)\n",
    "            if Node2.node_status == 'incomplete' or Node2.node_status == 'empty': \n",
    "                Node2.downloadNode()\n",
    "                call = call + 1\n",
    "                print(call)\n",
    "                if call >= callLimit: break\n",
    "\n",
    "    def getNodesEdges(self,threshold):\n",
    "        selected_nodes = [k for k,v in self.nodes_count.items() if float(v) >= threshold]\n",
    "        selected_edges = [(a,b) for a,b in self.network_edges if a in selected_nodes and b in selected_nodes]\n",
    "\n",
    "        nodes_network = []\n",
    "        for node in selected_nodes:\n",
    "            node_tuple = (node, {\"name\": node})\n",
    "            nodes_network.append(node_tuple)\n",
    "\n",
    "        return (nodes_network,selected_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(WikiNodesEdges):\n",
    "    \n",
    "    Graph = nx.Graph()\n",
    "\n",
    "    # Graph.add_nodes_from(WikiNodesEdges[0])\n",
    "    Graph.add_edges_from(WikiNodesEdges[1])\n",
    "\n",
    "    netdraw = Network('2000px', '2000px')\n",
    "    netdraw.from_nx(Graph)\n",
    "    netdraw.barnes_hut()\n",
    "\n",
    "    netdraw.show(\"wikigraphEN.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM HERE RUN TIME STARTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Succesfully loaded.\n"
     ]
    }
   ],
   "source": [
    "wikinet = WikiNetwork(\"Terrorism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikinet.downloadNetwork(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'complete': 781, 'dead': 4})"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "wikinet.getStatusNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_data = wikinet.getNodesEdges(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawGraph(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM HERE RUN TIME ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "    S = requests.Session()\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        # \"titles\": \"London\",\n",
    "        \"generator\": \"links&London\",\n",
    "        \"prop\": \"links\",\n",
    "        \"gplcontinue\": None, #NB: THIS SHOULD BE MAX PAGEID of result, then call AGAIN... \n",
    "        \"gplnamespace\": 0, # only load wikipedia main/articles. \n",
    "        \"gpllimit\": 500 # can go up to 500. Go for max? \n",
    "    }\n",
    "    response = S.get(url=API_ENDPOINT, params=PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_wiki = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'error': {'code': 'badgenerator',\n",
       "  'info': 'Unknown \"generator=links&London\".',\n",
       "  '*': 'See https://en.wikipedia.org/w/api.php for API usage. Subscribe to the mediawiki-api-announce mailing list at &lt;https://lists.wikimedia.org/mailman/listinfo/mediawiki-api-announce&gt; for notice of API deprecations and breaking changes.'},\n",
       " 'servedby': 'mw1412'}"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "data_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'plcontinue': '808|0|Academy_Award_for_Best_Actress', 'continue': '||'}"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "data_wiki['continue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'pageid': 37756, 'ns': 0, 'title': 'Delhi'}"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "data_wiki['query']['pages']['37756']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'from': 'Zevenaar ', 'to': 'Zevenaar'},\n",
       " {'from': ' Christianity ', 'to': 'Christianity'},\n",
       " {'from': ' Arnhem', 'to': 'Arnhem'}]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "data_wiki['query']['normalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}