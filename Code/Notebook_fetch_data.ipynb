{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# Setup packages and file paths \n",
                "import requests\n",
                "import pandas as pd\n",
                "\n",
                "path = \"/home/teijehidde/Documents/Git Blog and Coding/data_dump/\"\n",
                "data_file = \"data_new2.json\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "def downloadWikiNetwork(node_title, lang = \"en\"): # , additional_langs = [\"ar\" \"de\", \"fr\", \"nl\"]): \n",
                "\n",
                "    api_endpoint = \"https://\" + lang + \".wikipedia.org/w/api.php\" # fr.wikipedia.org; https://en.wikipedia.org\n",
                "    wiki_data = []\n",
                "    print(\"Starting download of network \" + node_title + \" in language \" + lang + \".\")\n",
                "    \n",
                "    # 1: download data on the node_title wikipage.\n",
                "    S = requests.Session()\n",
                "    params_node_title = {\n",
                "        \"action\": \"query\",\n",
                "        \"titles\": node_title,\n",
                "        \"prop\": \"info|links|langlinks\", \n",
                "        \"plnamespace\": 0, \n",
                "        \"pllimit\": 500,\n",
                "        \"lllimit\": 500, \n",
                "        \"format\": \"json\"\n",
                "    }\n",
                "    response = S.get(url=api_endpoint, params=params_node_title)\n",
                "    wiki_data.append(response.json())\n",
                "    \n",
                "    while 'continue' in wiki_data[-1].keys():\n",
                "        params_cont = params_node_title\n",
                "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
                "            params_cont[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
                "            print('plcontinue: ' + params_cont[\"plcontinue\"])\n",
                "\n",
                "        elif 'llcontinue' in wiki_data[-1]['continue']:\n",
                "            params_cont[\"llcontinue\"] = wiki_data[-1]['continue']['llcontinue'] \n",
                "            print('llcontinue: ' + params_cont[\"llcontinue\"])\n",
                "\n",
                "        response = S.get(url=api_endpoint, params=params_cont)\n",
                "        wiki_data.append(response.json())\n",
                "\n",
                "    # 2: download data on the links of node_title wikipage.\n",
                "    S = requests.Session()\n",
                "    params_network_title = {\n",
                "        \"action\": \"query\",\n",
                "        \"generator\": \"links\",\n",
                "        \"titles\": node_title,\n",
                "        \"gplnamespace\": 0, \n",
                "        \"gpllimit\": 500, \n",
                "        \"plnamespace\": 0,\n",
                "        \"pllimit\": 500, \n",
                "        \"prop\": \"info|links\", \n",
                "        \"format\": \"json\"\n",
                "    }\n",
                "    response = S.get(url=api_endpoint, params=params_network_title)\n",
                "    wiki_data.append(response.json())\n",
                "\n",
                "    while 'continue' in wiki_data[-1].keys():\n",
                "        params_cont = params_network_title\n",
                "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
                "            params_cont[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
                "            print('plcontinue: ' + params_cont[\"plcontinue\"])\n",
                "\n",
                "        elif 'gplcontinue' in wiki_data[-1]['continue']: \n",
                "            params_cont[\"gplcontinue\"] = wiki_data[-1]['continue']['gplcontinue']\n",
                "            print('gplcontinue: ' + params_cont[\"gplcontinue\"])\n",
                "\n",
                "        response = S.get(url=api_endpoint, params = params_cont)\n",
                "        wiki_data.append(response.json())\n",
                "\n",
                "    # 3: creating list of available nodes in wiki_data. \n",
                "    all_nodes = []\n",
                "\n",
                "    for item in wiki_data:\n",
                "        all_nodes = all_nodes + list(item['query']['pages'].keys())\n",
                "    all_nodes = list(set(all_nodes))\n",
                "    all_nodes = [int(i) for i in all_nodes if int(i) > 0]\n",
                "    all_nodes = [str(i) for i in all_nodes]\n",
                "    \n",
                "    network_data_df = pd.DataFrame(\n",
                "        columns = ['title', 'lang', 'pageid', 'uniqueid', 'lastrevid', 'links', 'langlinks'], # complete list: ['ns', 'title', 'missing', 'contentmodel', 'pagelanguage', 'pagelanguagehtmlcode', 'pageid', 'lastrevid', 'length', 'links', 'langlinks']\n",
                "        index = all_nodes)\n",
                "\n",
                "    # 4: Using all_nodes to go through list of raw data from API. \n",
                "    for node in all_nodes:\n",
                "        network_data_df.at[node,'links'] = []\n",
                "        for item in wiki_data:\n",
                "            if node in item['query']['pages'].keys(): # possibility:  df_new_wiki_data.update(item) #, errors = 'raise') \n",
                "                network_data_df.at[node, 'title'] = item['query']['pages'][node]['title']\n",
                "                network_data_df.at[node,'lang'] = item['query']['pages'][node]['pagelanguage']\n",
                "                network_data_df.at[node,'pageid'] = item['query']['pages'][node]['pageid']\n",
                "                network_data_df.at[node,'uniqueid'] = network_data_df.at[node,'lang'] + str(network_data_df.at[node,'pageid'])\n",
                "                network_data_df.at[node,'lastrevid'] = item['query']['pages'][node]['lastrevid']\n",
                "\n",
                "                if 'links' in item['query']['pages'][node].keys():\n",
                "                    links_temp = []\n",
                "                    for link in item['query']['pages'][node]['links']:\n",
                "                        links_temp.append(link['title'])\n",
                "                    network_data_df.at[node,'links'] + links_temp\n",
                "\n",
                "                if 'langlinks' in item['query']['pages'][node].keys():\n",
                "                    network_data_df.at[node,'langlinks'] = item['query']['pages'][node]['langlinks']\n",
                "\n",
                "    # returns panda with all data from network. \n",
                "    return (wiki_data, network_data_df)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Function: complete download of single title in multiple languages. - saves data in json file. \n",
                "def downloadMultiLangWikiNetwork(node_title, original_lang = 'en', additional_langs = [\"ar\", \"de\", \"fr\", \"nl\"]): # or: 'available_langs'\n",
                "    network_data_df = downloadWikiNetwork(node_title=node_title, lang=original_lang)\n",
                "    try: \n",
                "        available_langs = network_data_df.loc[network_data_df['langlinks'].notnull()]['langlinks'].values.tolist()[0]\n",
                "    except: \n",
                "        print(network_data_df.loc[network_data_df['title'] == node_title].loc[network_data_df['lang'] == original_lang]['langlinks'].values.tolist())\n",
                "        pass\n",
                "\n",
                "    if additional_langs == []:\n",
                "        print('The wikipedia page is available in the following languages:')         \n",
                "        print(available_langs)\n",
                "    \n",
                "    else:\n",
                "        for item in available_langs: \n",
                "            if item['lang'] in additional_langs:\n",
                "                network_data_df_additional = downloadWikiNetwork(node_title = item['*'], lang = item['lang'])\n",
                "                network_data_df = pd.concat([network_data_df, network_data_df_additional], ignore_index=True, sort=False)\n",
                "                \n",
                "    network_data_saved = pd.read_json((path + data_file), orient='split')\n",
                "    network_data_df = pd.concat([network_data_df, network_data_saved], ignore_index=True, sort=False)\n",
                "    network_data_df = network_data_df.loc[network_data_df.astype(str).drop_duplicates(keep = 'last').index].reset_index(drop=True)\n",
                "    network_data_df.to_json((path + data_file), orient='split')\n",
                "    print(\"Download of network and additional languages finished.\") \n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "def overviewWikiNetworks(): \n",
                "    network_data_df = pd.read_json((path + data_file), orient='split')\n",
                "\n",
                "    available_wiki_networks = network_data_df.loc[network_data_df['langlinks'].notnull()]\n",
                "    available_wiki_networks['number_of_links'] = available_wiki_networks['links'].apply(lambda x: len(x))\n",
                "\n",
                "    available_topics = network_data_df.loc[network_data_df['langlinks'].notnull()].loc[network_data_df['lang'] == 'en'][['title','langlinks']]\n",
                "    \n",
                "    # available_langs  = set(network_data_df['lang'])\n",
                "    # overview_df = pd.DataFrame(\n",
                "    # columns = available_langs, \n",
                "    # index = available_topics)\n",
                "\n",
                "    # available_wiki_networks[['title', 'lang', 'number_of_links']],.pivot(index = 'title', columns= 'lang', values = 'number_of_links')\n",
                "\n",
                "    return available_wiki_networks\n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "# Kubernetes, Vaccine, Oxford, [Verkhovna_Rada this one seems to have a bug - fix later; see also 'Consumer_protection'] \n",
                "# wiki_data_kubs = downloadWikiNetwork('Kubernetes')\n",
                "# wiki_data_Verkhovna_Rada = downloadWikiNetwork('Verkhovna_Rada')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "wiki_data = Vaccine_data[0]\n",
                "all_nodes = []\n",
                "\n",
                "for item in wiki_data:\n",
                "    all_nodes = all_nodes + list(item['query']['pages'].keys())\n",
                "all_nodes = list(set(all_nodes))\n",
                "all_nodes = [int(i) for i in all_nodes if int(i) > 0]\n",
                "all_nodes = [str(i) for i in all_nodes]\n",
                "\n",
                "network_data_df = pd.DataFrame(\n",
                "    columns = ['title', 'lang', 'pageid', 'uniqueid', 'lastrevid', 'links', 'langlinks'], # complete list: ['ns', 'title', 'missing', 'contentmodel', 'pagelanguage', 'pagelanguagehtmlcode', 'pageid', 'lastrevid', 'length', 'links', 'langlinks']\n",
                "    index = all_nodes)\n",
                "\n",
                "# 4: Using all_nodes to go through list of raw data from API. \n",
                "for node in all_nodes:\n",
                "    network_data_df.at[node,'links'] = []\n",
                "    for item in wiki_data:\n",
                "        if node in item['query']['pages'].keys(): # possibility:  df_new_wiki_data.update(item) #, errors = 'raise') \n",
                "            network_data_df.at[node, 'title'] = item['query']['pages'][node]['title']\n",
                "            network_data_df.at[node,'lang'] = item['query']['pages'][node]['pagelanguage']\n",
                "            network_data_df.at[node,'pageid'] = item['query']['pages'][node]['pageid']\n",
                "            network_data_df.at[node,'uniqueid'] = network_data_df.at[node,'lang'] + str(network_data_df.at[node,'pageid'])\n",
                "            network_data_df.at[node,'lastrevid'] = item['query']['pages'][node]['lastrevid']\n",
                "\n",
                "            if 'links' in item['query']['pages'][node].keys():\n",
                "                for link in item['query']['pages'][node]['links']:\n",
                "                    network_data_df.at[node,'links'].append(link['title'])\n",
                "\n",
                "            if 'langlinks' in item['query']['pages'][node].keys():\n",
                "                network_data_df.at[node,'langlinks'] = item['query']['pages'][node]['langlinks']"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "TypeError",
                    "evalue": "unsupported operand type(s) for +: 'float' and 'list'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-12-347658db92fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mlinks_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mnetwork_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlinks_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'langlinks'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'list'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "for item in wiki_data:\n",
                "    all_nodes = all_nodes + list(item['query']['pages'].keys())\n",
                "all_nodes = list(set(all_nodes))\n",
                "all_nodes = [int(i) for i in all_nodes if int(i) > 0]\n",
                "all_nodes = [str(i) for i in all_nodes]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "# all_nodes"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "for item in wiki_data['query']['pages']: \n",
                "    if '2646623' in item['query']['pages'].keys():\n",
                "       if 'links' in item['query']['pages']['2646623'].keys(): \n",
                "           print(item)"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "TypeError",
                    "evalue": "list indices must be integers or slices, not str",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-26-b5c4b6475aa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'2646623'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m'links'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pages'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2646623'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit"
        },
        "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}