{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 246,
            "source": [
                "# Setup packages and file paths \n",
                "import os\n",
                "from itertools import chain\n",
                "from collections import Counter\n",
                "import collections \n",
                "import requests\n",
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np \n",
                "import zipfile\n",
                "from io import BytesIO\n",
                "\n",
                "path = \"/home/teijehidde/Documents/Git Blog and Coding/data_dump/\"\n",
                "data_file = \"data_new2.json\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 263,
            "source": [
                "def downloadWikiNetwork(node_title, lang = \"en\"): # , additional_langs = [\"ar\" \"de\", \"fr\", \"nl\"]): \n",
                "\n",
                "    api_endpoint = \"https://\" + lang + \".wikipedia.org/w/api.php\" # fr.wikipedia.org; https://en.wikipedia.org\n",
                "    wiki_data = []\n",
                "    print(\"Starting download of network \" + node_title + \" in language \" + lang + \".\")\n",
                "    \n",
                "    # 1: download data on the node_title wikipage.\n",
                "    S = requests.Session()\n",
                "    params_node_title = {\n",
                "        \"action\": \"query\",\n",
                "        \"titles\": node_title,\n",
                "        \"prop\": \"info|links|langlinks\", \n",
                "        \"plnamespace\": 0, \n",
                "        \"pllimit\": 500,\n",
                "        \"lllimit\": 500, \n",
                "        \"format\": \"json\"\n",
                "    }\n",
                "    response = S.get(url=api_endpoint, params=params_node_title)\n",
                "    wiki_data.append(response.json())\n",
                "    \n",
                "    while 'continue' in wiki_data[-1].keys():\n",
                "        params_cont = params_node_title\n",
                "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
                "            params_cont[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
                "            print('plcontinue: ' + params_cont[\"plcontinue\"])\n",
                "\n",
                "        elif 'llcontinue' in wiki_data[-1]['continue']:\n",
                "            params_cont[\"llcontinue\"] = wiki_data[-1]['continue']['llcontinue'] \n",
                "            print('llcontinue: ' + params_cont[\"llcontinue\"])\n",
                "\n",
                "        response = S.get(url=api_endpoint, params=params_cont)\n",
                "        wiki_data.append(response.json())\n",
                "\n",
                "    # 2: download data on the links of node_title wikipage.\n",
                "    S = requests.Session()\n",
                "    params_network_title = {\n",
                "        \"action\": \"query\",\n",
                "        \"generator\": \"links\",\n",
                "        \"titles\": node_title,\n",
                "        \"gplnamespace\": 0, \n",
                "        \"gpllimit\": 500, \n",
                "        \"plnamespace\": 0,\n",
                "        \"pllimit\": 500, \n",
                "        \"prop\": \"info|links\", \n",
                "        \"format\": \"json\"\n",
                "    }\n",
                "    response = S.get(url=api_endpoint, params=params_network_title)\n",
                "    wiki_data.append(response.json())\n",
                "\n",
                "    while 'continue' in wiki_data[-1].keys():\n",
                "        params_cont = params_network_title\n",
                "        if 'plcontinue' in wiki_data[-1]['continue']:\n",
                "            params_cont[\"plcontinue\"] = wiki_data[-1]['continue']['plcontinue'] \n",
                "            print('plcontinue: ' + params_cont[\"plcontinue\"])\n",
                "\n",
                "        elif 'gplcontinue' in wiki_data[-1]['continue']: \n",
                "            params_cont[\"gplcontinue\"] = wiki_data[-1]['continue']['gplcontinue']\n",
                "            print('gplcontinue: ' + params_cont[\"gplcontinue\"])\n",
                "\n",
                "        response = S.get(url=api_endpoint, params = params_cont)\n",
                "        wiki_data.append(response.json())\n",
                "\n",
                "    # 3: creating list of available nodes in wiki_data. \n",
                "    all_nodes = []\n",
                "\n",
                "    for item in wiki_data:\n",
                "        all_nodes = all_nodes + list(item['query']['pages'].keys())\n",
                "    all_nodes = list(set(all_nodes))\n",
                "    all_nodes = [int(i) for i in all_nodes if int(i) > 0]\n",
                "    all_nodes = [str(i) for i in all_nodes]\n",
                "    \n",
                "    network_data_df = pd.DataFrame(\n",
                "        columns = ['title', 'lang', 'pageid', 'uniqueid', 'lastrevid', 'links', 'langlinks'], # complete list: ['ns', 'title', 'missing', 'contentmodel', 'pagelanguage', 'pagelanguagehtmlcode', 'pageid', 'lastrevid', 'length', 'links', 'langlinks']\n",
                "        index = all_nodes)\n",
                "\n",
                "    # 4: Using all_nodes to go through list of raw data from API. \n",
                "    for node in all_nodes:\n",
                "                \n",
                "        for item in wiki_data:\n",
                "            if node in item['query']['pages'].keys(): # possibility:  df_new_wiki_data.update(item) #, errors = 'raise') \n",
                "                network_data_df.at[node, 'title'] = item['query']['pages'][node]['title']\n",
                "                network_data_df.at[node,'lang'] = item['query']['pages'][node]['pagelanguage']\n",
                "                network_data_df.at[node,'pageid'] = item['query']['pages'][node]['pageid']\n",
                "                network_data_df.at[node,'uniqueid'] = network_data_df.at[node,'lang'] + str(network_data_df.at[node,'pageid'])\n",
                "                network_data_df.at[node,'lastrevid'] = item['query']['pages'][node]['lastrevid']\n",
                "\n",
                "                if 'links' in item['query']['pages'][node].keys():\n",
                "                    links_temp = []\n",
                "                    for link in item['query']['pages'][node]['links']:\n",
                "                        links_temp.append(link['title'])\n",
                "                    network_data_df.at[node,'links'] = links_temp\n",
                "\n",
                "                if 'langlinks' in item['query']['pages'][node].keys():\n",
                "                    network_data_df.at[node,'langlinks'] = item['query']['pages'][node]['langlinks']\n",
                "\n",
                "    # returns panda with all data from network. \n",
                "    return network_data_df\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 261,
            "source": [
                "# Function: complete download of single title in multiple languages. - saves data in json file. \n",
                "def downloadMultiLangWikiNetwork(node_title, original_lang = 'en', additional_langs = [\"ar\", \"de\", \"fr\", \"nl\"]): # or: 'available_langs'\n",
                "    network_data_df = downloadWikiNetwork(node_title=node_title, lang=original_lang)\n",
                "    available_langs = network_data_df.loc[network_data_df['title'] == node_title].loc[network_data_df['lang'] == original_lang]['langlinks'].values.tolist()[0]\n",
                "\n",
                "    if additional_langs == []:\n",
                "        print('The wikipedia page is available in the following languages:')         \n",
                "        print(available_langs)\n",
                "    \n",
                "    else:\n",
                "        for item in available_langs: \n",
                "            if item['lang'] in additional_langs:\n",
                "                network_data_df_additional = downloadWikiNetwork(node_title = item['*'], lang = item['lang'])\n",
                "                network_data_df = pd.concat([network_data_df, network_data_df_additional], ignore_index=True, sort=False)\n",
                "                \n",
                "    network_data_saved = pd.read_json((path + data_file), orient='split')\n",
                "    network_data_df = pd.concat([network_data_df, network_data_saved], ignore_index=True, sort=False)\n",
                "    network_data_df = network_data_df.loc[network_data_df.astype(str).drop_duplicates(keep = 'last').index].reset_index(drop=True)\n",
                "    network_data_df.to_json((path + data_file), orient='split')\n",
                "    print(\"Download of network and additional languages finished.\") \n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 326,
            "source": [
                "def overviewWikiNetworks(): \n",
                "    network_data_df = pd.read_json((path + data_file), orient='split')\n",
                "\n",
                "    available_topics = network_data_df.loc[network_data_df['langlinks'].notnull()].loc[network_data_df['lang'] == 'en']['title']\n",
                "    available_langs  = set(network_data_df['lang'])\n",
                "\n",
                "    overview_df = pd.DataFrame(\n",
                "    columns = available_langs, \n",
                "    index = available_topics)\n",
                "\n",
                "    for topic in available_topics:\n",
                "        langs_pd = pd.DataFrame(network_data_df.loc[network_data_df['title'] == topic].loc[network_data_df['lang'] == 'en']['langlinks'].values.tolist()[0])\n",
                "        for lang in available_langs: \n",
                "            try: \n",
                "                overview_df.at[topic,lang] = len(network_data_df.loc[network_data_df['title'] == langs_pd.loc[langs_pd['lang'] == lang]['*']].loc[network_data_df['lang'] == lang]['links'].values.tolist()[0])\n",
                "            except:\n",
                "                overview_df.at[topic,lang] = 'None'\n",
                "\n",
                "    return(overview_df)\n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 274,
            "source": [
                "# Kubernetes, Vaccine, Oxford, [Verkhovna_Rada this one seems to have a bug - fix later.] \n",
                "# wiki_data_kubs = downloadWikiNetwork('Kubernetes')\n",
                "# wiki_data_Verkhovna_Rada = downloadWikiNetwork('Verkhovna_Rada')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 333,
            "source": [
                "# downloadMultiLangWikiNetwork('Vaccine')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 332,
            "source": [
                "overviewWikiNetworks()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "              fr    en    de    nl    ar\n",
                            "title                                   \n",
                            "Vaccine     None  None  None  None  None\n",
                            "Secularism  None  None  None  None  None\n",
                            "Kubernetes  None  None  None  None  None"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>fr</th>\n",
                            "      <th>en</th>\n",
                            "      <th>de</th>\n",
                            "      <th>nl</th>\n",
                            "      <th>ar</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>title</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>Vaccine</th>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Secularism</th>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Kubernetes</th>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "      <td>None</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 332
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 434,
            "source": [
                "network_data_df = pd.read_json((path + data_file), orient='split')\n",
                "\n",
                "available_topics = network_data_df.loc[network_data_df['langlinks'].notnull()].loc[network_data_df['lang'] == 'en']['title']\n",
                "available_langs  = set(network_data_df['lang'])\n",
                "\n",
                "overview_df = pd.DataFrame(\n",
                "columns = available_langs, \n",
                "index = available_topics)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 435,
            "source": [
                "overview_df"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "             fr   en   de   nl   ar\n",
                            "title                              \n",
                            "Vaccine     NaN  NaN  NaN  NaN  NaN\n",
                            "Secularism  NaN  NaN  NaN  NaN  NaN\n",
                            "Kubernetes  NaN  NaN  NaN  NaN  NaN"
                        ],
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>fr</th>\n",
                            "      <th>en</th>\n",
                            "      <th>de</th>\n",
                            "      <th>nl</th>\n",
                            "      <th>ar</th>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>title</th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "      <th></th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>Vaccine</th>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Secularism</th>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>Kubernetes</th>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "      <td>NaN</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 435
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 443,
            "source": [
                "for topic in available_topics.values.tolist():\n",
                "    langs_topic_pd = network_data_df.loc[network_data_df['title'] == topic].loc[network_data_df['lang'] == 'en']['langlinks'].values.tolist()[0]\n",
                "\n",
                "    try: \n",
                "        for lang in available_langs:\n",
                "            localized_title = [i['*'] for i in langs_topic_pd if i['lang'] == lang][0]\n",
                "            overview_df[topic,lang] = len(network_data_df.loc[network_data_df['title'] == localized_title].loc[network_data_df['lang'] == lang]['links'].values.tolist()[0]) \n",
                "    except:\n",
                "        overview_df[topic,lang] = 'nan'\n",
                "\n",
                "    # langs_topic_pd\n",
                "\n",
                "    #  try: \n",
                "    #        overview_df.at[topic,lang] = len(network_data_df.loc[network_data_df['title'] == langs_pd.loc[langs_pd['lang'] == lang]['*']].loc[network_data_df['lang'] == lang]['links'].values.tolist()[0])\n",
                "    #    except:\n",
                "    #        overview_df.at[topic,lang] = 'None'"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "133\n",
                        "214\n",
                        "80\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 442,
            "source": [
                "available_topics.values.tolist()\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "['Vaccine', 'Secularism', 'Kubernetes']"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 442
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 325,
            "source": [
                "langs_pd.loc[langs_pd['lang'] == 'fr']['*']\n"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "27    Laïcité\n",
                            "Name: *, dtype: object"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 325
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "# test.iloc[10:80,]"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.10 64-bit"
        },
        "interpreter": {
            "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}